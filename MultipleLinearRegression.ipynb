{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44645ebd",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "Multiple Linear Regression (MLR) is a fundamental statistical method in Data Science used to model the relationship between a dependent variable (target) and multiple independent variables (features). Unlike Simple Linear Regression, MLR allows us to account for multiple predictors simultaneously, making it more powerful for real-world datasets where multiple factors influence the outcome. \n",
    "\n",
    "Since in the [Linear Regression](LinearRegression.ipynb) notebook we covered the more basic concepts, here we will delve into sconcepts such as the changed key metrics (such as Adjusted R-quared), testing the significance of the model, OLS Assumptions and Dummy Variables.\n",
    "\n",
    "## Index\n",
    "\n",
    "\n",
    "**Libraries used:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800b558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from scipy.stats import probplot\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086fcd7b",
   "metadata": {},
   "source": [
    "## 1. The Multiple Linear Regression Model\n",
    "\n",
    "The formula for the MLR is an extension form of the simple Linear Regression formula:\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\n",
    "$$\n",
    "\n",
    "where Y is the target (dependent) variable, $X_1, X_2, ..., X_p$ are the features (independent variables), $\\beta_0$ is the intercept term, $\\beta_1, \\beta_2, ..., \\beta_p$ are the coefficients and $\\epsilon$ is the error term (residuals).\n",
    "\n",
    "For this notebook, we will work on seaborns **California Housing Dataset**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "865dfd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            MedHouseVal   R-squared:                       0.606\n",
      "Model:                            OLS   Adj. R-squared:                  0.606\n",
      "Method:                 Least Squares   F-statistic:                     3970.\n",
      "Date:                Wed, 16 Apr 2025   Prob (F-statistic):               0.00\n",
      "Time:                        16:03:17   Log-Likelihood:                -22624.\n",
      "No. Observations:               20640   AIC:                         4.527e+04\n",
      "Df Residuals:                   20631   BIC:                         4.534e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -36.9419      0.659    -56.067      0.000     -38.233     -35.650\n",
      "MedInc         0.4367      0.004    104.054      0.000       0.428       0.445\n",
      "HouseAge       0.0094      0.000     21.143      0.000       0.009       0.010\n",
      "AveRooms      -0.1073      0.006    -18.235      0.000      -0.119      -0.096\n",
      "AveBedrms      0.6451      0.028     22.928      0.000       0.590       0.700\n",
      "Population -3.976e-06   4.75e-06     -0.837      0.402   -1.33e-05    5.33e-06\n",
      "AveOccup      -0.0038      0.000     -7.769      0.000      -0.005      -0.003\n",
      "Latitude      -0.4213      0.007    -58.541      0.000      -0.435      -0.407\n",
      "Longitude     -0.4345      0.008    -57.682      0.000      -0.449      -0.420\n",
      "==============================================================================\n",
      "Omnibus:                     4393.650   Durbin-Watson:                   0.885\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            14087.596\n",
      "Skew:                           1.082   Prob(JB):                         0.00\n",
      "Kurtosis:                       6.420   Cond. No.                     2.38e+05\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.38e+05. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "california = fetch_california_housing()\n",
    "df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "df['MedHouseVal'] = california.target\n",
    "\n",
    "# Fit MLR model\n",
    "model = ols('MedHouseVal ~ MedInc + HouseAge + AveRooms + AveBedrms + Population + AveOccup + Latitude + Longitude', data=df).fit()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbbb662",
   "metadata": {},
   "source": [
    "## 2. Adjusted R-squared\n",
    "\n",
    "Since $R^2$ measures the proportion of variance explained by the model, the adjusted version of this metric penalizes the aaddition of unnecessary predictors:\n",
    "\n",
    "$$\n",
    "Adjusted R^2 = 1 - \\frac{(1- R^2) (n-1)}{n-p-1}\n",
    "$$\n",
    "\n",
    "where $n$ is the number of observations and $p$ is the number of predictors. A higher Adjusted $R^2$ assumes a better model fit and is preferable over the simple $R^2$ when comparing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87061f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6062\n",
      "Adjusted R-squared: 0.6061\n"
     ]
    }
   ],
   "source": [
    "print(f\"R-squared: {model.rsquared:.4f}\")\n",
    "print(f\"Adjusted R-squared: {model.rsquared_adj:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
